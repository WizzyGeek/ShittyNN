{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory about the wacky things I was trying\n",
    "\n",
    "*wait till i figure out mathjax*<br />\n",
    "*please ignore my not so concrete definations*\n",
    "\n",
    "## Linear Classifier training shit\n",
    "\n",
    "Let D be the set of training inputs such that\n",
    "\n",
    "$$\\renewcommand{\\R}{\\mathbb{R}}\n",
    "D \\subseteq A \\subseteq \\R^k$$\n",
    "\n",
    "Where the perceptron is defined as\n",
    "\n",
    "$$\n",
    "f(\\vec{x}): A \\to \\R=\\sigma(\\left(\\vec{\\omega}\\ \\cdot\\ \\vec{x}\\right) + b)$$\n",
    "\n",
    "Here $\\sigma(x): \\mathbb{R} \\to \\mathbb{R}$ is the activation function\n",
    "used by the perceptron, $A$ is the set of all possible inputs.\n",
    "\n",
    "$\\vec{x}$ is any vector input accepted by the perceptron,\n",
    "$\\vec{\\omega}$ is the vector of weights $(\\omega_1,\\omega_2,\\dots,\\omega_k)$\n",
    "and $b$ is the bias\n",
    "\n",
    "Now going by the delta rule which is a special case of gradient descent\n",
    "\n",
    "$$ E (\\vec{\\omega}) = {\\frac12}\\sum_{\\vec{x} \\in D} \\left[ t(\\vec{x}) - f(\\vec{x})\\right]^2$$\n",
    "where $t(\\vec{x})$ is the desired value for $\\vec{x}$\n",
    "\n",
    "we can easily exploit the chain rule to the point a monkey can find the gradient now\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\pfrac}[2]{{\\frac{\\partial#1}{\\partial#2}}}\n",
    "\\renewcommand{\\oink}{\\omega_i}\n",
    "\n",
    "\\begin{align*}\n",
    "\\pfrac{ E (\\vec{\\omega})}{\\omega_i} & = \\frac12\\sum\\pfrac{\\left\n",
    "[t(\\vec{x}) - f(\\vec{x})\\right]^2}{\\left[ t(\\vec{x}) - f(\\vec{x})\\right]}\n",
    "\\times\\pfrac{\\left[ t(\\vec{x}) - f(\\vec{x})\\right]}{f(\\vec{x})}\\times\n",
    "\\pfrac{f(\\vec{x})}{\\left(\\vec{\\omega}\\ \\cdot\\ \\vec{x}\\right) + b}\\times\n",
    "\\pfrac{\\left(\\vec{\\omega}\\ \\cdot\\ \\vec{x}\\right) + b}{\\oink} \\\\\n",
    "& = \\sum_{\\vec{x}\\in D}[t(\\vec{x})-f(\\vec{x})]\\times-1\\times\\sigma'(\\vec{x}\\cdot\\vec{\\omega} + b)\\times\n",
    "x_i \\\\\n",
    "& = -\\sum_{\\vec{x}\\in D}\\sigma'(z(\\vec{x}))\\times[t(\\vec{x})-f(\\vec{x})]\\times x_i\n",
    "\\end{align*}\n",
    "\\\\\n",
    "\\text{\n",
    "    Where, $z(\\vec{x}) = \\vec{x}\\cdot\\vec{\\omega} + b$\n",
    "}\n",
    "$$\n",
    "\n",
    "finally we can say $\\Delta\\omega_i = \\eta\\sum\\sigma'(z(\\vec{x}))\\cdot[t(\\vec{x})-f(\\vec{x})]\\cdot x_i$ to update the i-th weight\n",
    "\n",
    "where $\\eta$ is the learning rate\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\ipart}[1]{\\frac{\\partial E (\\vec{\\omega})}{\\partial\\omega_#1}}\n",
    "\\nabla E (\\vec{\\omega}) = \\left[\\ipart1,\\ipart2,\\dots,\\ipart{k}\\right]\n",
    "\\\\[2ex]\\& \\\\[1ex]\n",
    "\\Delta\\vec\\omega = -\\eta\\nabla E (\\vec{\\omega}) \\\\[1ex]\\& \\\\[1ex]\n",
    "\\vec\\omega \\leftarrow \\vec\\omega + \\Delta\\vec\\omega\n",
    "$$\n",
    "\n",
    "## Questions I just cant understand\n",
    "\n",
    "- How in the world does this converge?\n",
    "\n",
    "Let $D$, the training set be a singleton set\n",
    "$$\n",
    "\\begin{align*}\n",
    "& \\sigma(z) = z \\implies \\sigma'(z) = 1\\\\\n",
    "& \\implies\\Delta\\vec\\omega_i = \\eta\\cdot[t(\\vec{x})-f(\\vec{x})]\\cdot x_i\\\\[1ex]\n",
    "& \\text{Assume that $t(\\vec x) = \\omega_{target} \\cdot \\vec x$ and\n",
    "$(\\forall \\omega_{target,k} \\in \\omega_{target} \\mid k \\not= i)\\rightarrow(\n",
    "    \\omega_k = \\omega_{target,k}\n",
    ")$} \\\\[1ex]\n",
    "& \\implies\\Delta\\vec\\omega_i = \\eta\\cdot x_i^2 \\cdot[\\omega_{target,i}-\\omega_i]\\\\\n",
    "& \\text{Applying to GD and IGD algorithms} \\\\\n",
    "& \\omega_i \\leftarrow \\omega_i + (\\omega_{target,i} - \\omega_i)*\\eta*x^2\\\\[1ex]\n",
    "& \\text{Clearly this converges $\\omega_i$ to $\\omega_{target,i}$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "`GD` = Gradient descent,\n",
    "`IGD` = Incremental Gradient Descent\n",
    "\n",
    "if you scale the above example to a bigger dataset then $\\omega_i$ will aproach the\n",
    "the mean value needed to fit every input.\n",
    "\n",
    "I dont have any expertise in this topic so I dunno if it's correct at all or\n",
    "how to show that it becomes an approximation in other cases\n",
    "\n",
    "## Lab\n",
    "\n",
    "- What if you change the error function to use absolute value of odd powers?\n",
    "- What if you change the error function to use other even powers?\n",
    "- What if you change it to use nth-roots somehow?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
